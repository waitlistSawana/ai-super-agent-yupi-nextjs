{
  "$schema": "https://ui.shadcn.com/schema/registry-item.json",
  "name": "nextjs-chat-bot-with-middleware",
  "type": "registry:page",
  "title": "NextJS Chat Bot with LLM Middleware",
  "description": "An advanced chat bot implementation with LLM middleware support. Includes UI component, API endpoint, and middleware handlers for enhanced AI interactions. Built with Vercel AI SDK.",
  "dependencies": [
    "ai",
    "@ai-sdk/react"
  ],
  "files": [
    {
      "path": "src/lib/ai/llm-middleware/dashscope.ts",
      "content": "/**\n * Language model middleware\n *\n * Language model middleware is a way to enhance the behavior\n * of language models by intercepting and modifying the calls\n * to the language model.\n *\n * build with dashscope\n *\n * @see https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\n * @see https://ai-sdk.dev/docs/ai-sdk-core/middleware\n *\n * @author Sawana Huang <hsawana9@gmail.com>\n * @date 2025-05-03\n *\n */\nimport { dashscope } from \"@/lib/ai/providers/dashscope\";\n\nimport {\n  wrapLanguageModel,\n  extractReasoningMiddleware,\n  simulateStreamingMiddleware,\n  defaultSettingsMiddleware,\n  type LanguageModelV1Middleware,\n  type LanguageModelV1StreamPart,\n} from \"ai\";\n\n/**\n * Built-in Middlewares\n *\n */\n\n/**\n * Extract Reasoning Middleware\n *\n * Some providers and models expose reasoning information\n * in the generated text using special tags,\n * e.g. <think> and </think>.\n */\nconst extractReasoningModel = wrapLanguageModel({\n  model: dashscope(\"deepseek-r1\"),\n  middleware: extractReasoningMiddleware({ tagName: \"think\" }),\n});\n\n/**\n * Simulate Streaming Middleware\n *\n * Be used to simulate streaming behavior with responses from non-streaming language models\n */\nconst simulateStreamingModel = wrapLanguageModel({\n  model: dashscope(\"deepseek-v3\"),\n  middleware: simulateStreamingMiddleware(),\n});\n\n/**\n * Default Settings Middleware\n *\n * Apply default settings to a language model.\n */\nconst defaultSettingsModel = wrapLanguageModel({\n  model: dashscope(\"deepseek-v3\"),\n  middleware: defaultSettingsMiddleware({\n    settings: {\n      temperature: 0.5,\n      maxTokens: 800,\n      // note: use providerMetadata instead of providerOptions here:\n      providerMetadata: { openai: { store: false } },\n    },\n  }),\n});\n\n/**\n * Implementing Language Model Middleware\n *\n * customize your own middleware\n *\n * - `transformParams`: Modify the parameters of the language model call.\n * - `wrapGenerate`: wrap the doGenerate function.\n * - `wrapStream`: wrap the doStream function.\n *\n * you can modify the paramers, call the languaage moel,\n * and modify the result. in other words, you can do someting\n * before and after the language model call.\n *\n * following are examples from ai adk\n */\n\n/**\n * Logging Middleware\n */\nconst loggingMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    console.log(\"doGenerate called\");\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n    const result = await doGenerate();\n\n    console.log(\"doGenerate finished\");\n    console.log(`generated text: ${result.text}`);\n\n    return result;\n  },\n\n  wrapStream: async ({ doStream, params }) => {\n    console.log(\"doStream called\");\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n    const { stream, ...rest } = await doStream();\n\n    let generatedText = \"\";\n\n    const transformStream = new TransformStream<\n      LanguageModelV1StreamPart,\n      LanguageModelV1StreamPart\n    >({\n      transform(chunk, controller) {\n        if (chunk.type === \"text-delta\") {\n          generatedText += chunk.textDelta;\n        }\n\n        controller.enqueue(chunk);\n      },\n\n      flush() {\n        console.log(\"doStream finished\");\n        console.log(`generated text: ${generatedText}`);\n      },\n    });\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\n\nconst loggingModel = wrapLanguageModel({\n  model: dashscope(\"deepseek-v3\"),\n  middleware: loggingMiddleware,\n});\n\n/**\n * Guardrails\n */\nconst GuardrailMiddleware: LanguageModelV1Middleware = {\n  wrapGenerate: async ({ doGenerate }) => {\n    const { text, ...rest } = await doGenerate();\n\n    // filtering approach, e.g. for PII or other sensitive information:\n    const cleanedText = text?.replace(/badword/g, \"<REDACTED>\");\n\n    return { text: cleanedText, ...rest };\n  },\n\n  // here you would implement the guardrail logic for streaming\n  // Note: streaming guardrails are difficult to implement, because\n  // you do not know the full content of the stream until it's finished.\n};\n\nconst GuardrailModel = wrapLanguageModel({\n  model: dashscope(\"deepseek-v3\"),\n  middleware: GuardrailMiddleware,\n});\n\n/**\n * Public API\n */\nexport {\n  // built-in middlewares\n  extractReasoningModel,\n  simulateStreamingModel,\n  defaultSettingsModel,\n  // custom middlewares\n  loggingModel,\n  GuardrailModel,\n};\n",
      "type": "registry:lib",
      "target": "./lib/ai/llm-middleware/dashscope.ts"
    },
    {
      "path": "src/components/chat-bot-with-middleware.tsx",
      "content": "/**\n * comprehensive chatbot component\n *\n * @description: comprehensive chatbot component, following the document of AI SDK\n *\n * @see https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\n *\n * @author Sawana Huang <hsawana9@gmail.com>\n * @date 2025-05-03\n *\n */\n\n\"use client\";\n\nimport { cn } from \"@/lib/utils\";\n\nimport { useChat } from \"@ai-sdk/react\";\n\ntype ChatBotWithMiddlewareProps = {\n  className?: string;\n};\n\nexport default function ChatBotWithMiddleware({\n  className,\n  ...props\n}: React.ComponentProps<\"div\"> & ChatBotWithMiddlewareProps) {\n  const {\n    messages,\n    input,\n    handleInputChange,\n    handleSubmit,\n    status,\n    stop,\n    error,\n    reload,\n  } = useChat({\n    // API endpoint Default /api/chat\n    api: \"/api/v1/chat-bot-with-middleware\",\n    // Throttle the messages and data updates to 50ms:\n    experimental_throttle: 50,\n    // Event Callbacks:\n    // https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#event-callbacks\n    onFinish: (message, { usage, finishReason }) => {\n      console.log(\"Finished streaming message:\", message);\n      console.log(\"Token usage:\", usage);\n      console.log(\"Finish reason:\", finishReason);\n    },\n    onError: (error) => {\n      console.error(\"An error occurred:\", error);\n    },\n    onResponse: (response) => {\n      console.log(\"Received HTTP response from server:\", response);\n      // You can throw error here to trigger the onError callback\n    },\n    // Custom headers, body, and credentials\n    headers: {\n      Authorization: \"Bear your_token\",\n    },\n    body: {\n      key_use_chatbot: \"key of useChatbot\",\n    },\n    credentials: \"same-origin\",\n  });\n\n  return (\n    <div className={cn(\"\", className)} {...props}>\n      {messages.map((message) => (\n        <div key={message.id}>\n          {message.role === \"user\" ? \"User: \" : \"AI: \"}\n          {message.content}\n        </div>\n      ))}\n\n      {(status === \"submitted\" || status === \"streaming\") && (\n        <div>\n          {status === \"submitted\" && \"loading... \"}\n          <button type=\"button\" onClick={() => stop()}>\n            Stop\n          </button>\n        </div>\n      )}\n\n      {error && (\n        <div>\n          <div>An error occurred.</div>\n          <button type=\"button\" onClick={() => reload()}>\n            Retry\n          </button>\n        </div>\n      )}\n\n      <form\n        onSubmit={(event) => {\n          handleSubmit(event, {\n            // Custom body fieds per request\n            body: {\n              key_handle_submit: \"key of handleSubmit\",\n            },\n          });\n        }}\n      >\n        <input\n          name=\"prompt\"\n          value={input}\n          placeholder=\"Type your message here...\"\n          onChange={handleInputChange}\n          disabled={status !== \"ready\" || error != null}\n        />\n        <button type=\"submit\">Submit</button>\n      </form>\n    </div>\n  );\n}\n",
      "type": "registry:component"
    },
    {
      "path": "src/app/api/v1/chat-bot-with-middleware/route.ts",
      "content": "/**\n * server route for comprehensive chatbot component with language model middleware\n *\n * @see https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\n * @see https://ai-sdk.dev/docs/ai-sdk-core/middleware\n *\n * @author Sawana Huang <hsawana9@gmail.com>\n * @date 2025-05-03\n *\n */\n\nimport { loggingModel } from \"@/lib/ai/llm-middleware/dashscope\";\nimport { streamText, type UIMessage } from \"ai\";\nimport { type NextRequest } from \"next/server\";\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\nexport interface PostRequestBody {\n  messages: UIMessage[];\n}\nexport interface PostSuccessResponse {\n  message: string;\n}\n\nexport async function POST(request: NextRequest) {\n  const {\n    messages,\n    // custom fields\n    key_use_chatbot,\n    key_handle_submit,\n  } = (await request.json()) as PostRequestBody & {\n    // custom fields\n    key_use_chatbot: string;\n    key_handle_submit: string;\n  };\n\n  // custom fields\n  console.log({ key_use_chatbot, key_handle_submit });\n\n  const result = streamText({\n    // You can use any model you want.\n    // see: https://ai-sdk.dev/providers/ai-sdk-providers\n    model: loggingModel,\n    system: \"You are a helpful assistant.\",\n    messages,\n  });\n\n  return result.toDataStreamResponse({\n    // Error Message:\n    // The default error message is masked \"An error occurred.\"\n    // You can forward error messages or send custome messages.\n    getErrorMessage: (error) => {\n      if (error == null) {\n        return \"unknown error\";\n      }\n      if (typeof error === \"string\") {\n        return error;\n      }\n      if (error instanceof Error) {\n        return error.message;\n      }\n      return JSON.stringify(error);\n    },\n    // Usage Information\n    sendUsage: true,\n    // Reasoning:\n    // https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#reasoning\n    sendReasoning: true,\n    // Source\n    // https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#sources\n    // some providers response include sources\n    // eg. Perplexity, Google generative AI\n    sendSources: true,\n  });\n}\n",
      "type": "registry:page",
      "target": "./app/api/v1/chat-bot-with-middleware/route.ts"
    }
  ]
}